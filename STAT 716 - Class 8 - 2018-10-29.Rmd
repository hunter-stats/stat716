---
title: "STAT 716 - Class 8 - 2018-10-29"
author: "Vitaly Druker"
output: html_document
---
 
 
```{r}
library(tidyverse)
```


# Introduction

Improvements on least squares
 - Prediction Accuracy (if n is not >> p)
 - Interpertability (cubic terms?)
 
 This is called: feature selection, variable selection
 
 We are going to talk about 3 methods
 
 1. Subset Selection
 2. Shrinkage
 3. Dimension Reduction
 
 
# Subset Selection

First idea - let's look at all combinations possible.

 If we have p predictors - how many models are there?
 
 $\binom{p}{k}$
  
where k runs from 1 to p.

```{r}
total_params <- 2

# start from 0 because of the null model
total_models <- sapply(0:total_params, function(x){
  choose(total_params, x)
}) 

sum(total_models) 

2^total_params
```
 
 Issues with measure of accuracy
 
 
RSS, R^2 and MSE when number of predictors is the same is ok, but it does not work when number of predictors changes.

We therefore have to use AIC, BIC or cross validation or adjusted R^2.

## $C_p$

$$
C_p = \frac{1}{n}(RSS + 2d\hat\sigma^2)
$$
 where sigma^2 is the irreducible error. This is supposed to be an unbiased error of the MSE.
 
 However I don't think I've ever seen it used in practice.
 
## AIC
 
 It's proportional to $C_p$ but defined for more than just least squares.
 
 
 $$
 \frac{1}{n\hat\sigma^2}(RSS + 2d\hat\sigma^2)
 $$
 
## BIC

$$
\frac{1}{n}(RSS + log(n)d\hat\sigma^2)
$$
BIC places a heavier penality on models with more parameters - so fewer predictors are used.

## Adjusted R^2

$$
R^2 = 1 - RSS/TSS
$$
A large R^2 is wanted, as opposed to the other methods which want a small number.

$$
Adjusted R^2 = 1 - \frac{RSS/(n-d-1)}{TSS/(n-1)}
$$

# Shrinkage Methods


Linear modeling minimizes RSS:

$$
RSS = \sum_{i = 1}^{n}{(y_i - \beta_0 - \sum_{j = 1}^p{\beta_jx_{ij}})}
$$
What are all of the terms in the equation?

## Ridge Regression

Ridge regression instead penalizes:
$$
\sum_{i = 1}^{n}{(y_i - \beta_0 - \sum_{j = 1}^p{\beta_jx_{ij}})} + \lambda\sum_{j = 1}^p\beta^2_j
$$

$\lambda$ is a tuning parameter. Have we seen any other tuning parameters? What happens with $\lambda$ = 0.

It is _not_ applied to the intercept.

What happens when the x values are a different scale?
e.g. let's say you fit a model to dollars
If you then fit the model to thousands of dollars how would beta change?

It's important to scale and center the variables before applying the ridge regression.

Why does it improve? bias-variance trade off.

Thoughts on disadvantages?
$\beta$s are never actually zero.

## Lasso

Instead minimizes

$$
\sum_{i = 1}^{n}{(y_i - \beta_0 - \sum_{j = 1}^p{\beta_jx_{ij}})} + \lambda\sum_{j = 1}^p|\beta_j|
$$
This is the l1 penalty as opposed to the l2 penality shown above.

## Baysian View Point

$$
p(\beta|X,Y) \propto f(Y|X,\beta)p(\beta|X) = f(Y|X,\beta)p(\beta)
$$
If p(beta) is gaussian then youget ridge regression.
if g is a Laplace distribution the you get a pointed distribution.


You can also mix the two (alpha * l1 = (1-alpha * l2))

# Dimension Reduction

## Principal Components Regression

What is PCA?

Each component has the greatest amount of variance available to it.

scatter plot on a diagonal  - how does the diagnoal line capture most of the variance ( see page 230 of the text)

PCR is just PCA followed by regression.

Tuning parameter: number of priniciple componensts. It makes the assumption that the most variability in x is related to y?

Any example where high variability of x does not depend on y?

## Partial Least Squares

It's like fitting multiple linear regression, one after another on the residuals of the previous method.


# Examples

## Best Subset Selection

```{r}
library(ISLR)
library(leaps)


d_hit <- Hitters %>% 
  filter(complete.cases(.))


regfit_full <- regsubsets(Salary ~ ., d_hit, nvmax = 19)
summary(regfit_full)
```

```{r}
reg_fit_summary <- summary(regfit_full)

reg_fit_summary
```

```{r}
library(tidyr)
data.frame(
  vars = 1:length(reg_fit_summary$rss),
  rss = reg_fit_summary$rss,
  adjr2 = reg_fit_summary$adjr2,
  cp = reg_fit_summary$cp,
  bic = reg_fit_summary$bic) %>%
  # put data into 'long' format
  gather("error", "val",-vars) %>%
  group_by(error) %>%
  # label the best value
  mutate(best_val = ifelse(error %in% c("adjr2"),  
                           val == max(val), # we want the max adjr2
                           val == min(val))) %>% # This is for all other errors
  mutate(label = ifelse(best_val, vars, NA)) %>%
  ggplot(aes(x = vars, y = val, label = label)) +
  geom_line() + facet_wrap( ~ error, scales = "free") +
  geom_label()

```


```{r}

regfit_forward <- regsubsets(Salary ~ ., d_hit, nvmax = 19, method = "forward")
plot(regfit_forward, scale = "Cp")

full_mod <- lm(Salary ~ .,  data = d_hit)
null_mod <- lm(Salary ~ 1, data = d_hit)
step(null_mod, list(
  upper = lm(Salary ~ .,  data = d_hit),
  lower = lm(Salary ~ 1,  data = d_hit)), 
  direction = "both")
```

## Picking using CV

```{r}
library(caret)


train_object <- train(
  Salary ~ .,
  data = d_hit,
  method = "leapSeq",
  trControl = trainControl(
    method = "repeatedcv",
    number = 10,
    repeats = 5,
    returnResamp = "all",
    selectionFunction = "oneSE"
  ),
  tuneGrid = data.frame(nvmax = 1:19)
)

resamps <- length(train_object$control$index)
train_object$results %>%
  ggplot(aes(
    x = nvmax,
    y = RMSE,
    ymin = RMSE - RMSESD / sqrt(resamps),
    ymax = RMSE + RMSESD / sqrt(resamps)
  )) +
  geom_pointrange()

```



